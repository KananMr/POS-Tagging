{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7968c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   sentence          labels\n",
      "0           O, qırmızı, böyük maşını sürdü.  un ad ad nn un\n",
      "1  Bu, isti, qəhvə, səhərimi gözəlləşdirdi.  un ad nn un un\n",
      "2        Bu, kiçik, taxta qutu, xəzinəmdir.  un ad ad nn nn\n",
      "3          O, sakit, dəniz sahilində gəzdi.  un ad nn un un\n",
      "4        Böyük, dəmir qapı, qalanı qoruyur.  ad ad nn nn un\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e681eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return sentence.replace('\"', '').replace('.', '').replace(',', '').replace('?', '').split()\n",
    "\n",
    "df['tokens'] = df['sentence'].apply(tokenize)\n",
    "df['pos'] = df['labels'].apply(lambda x: x.split())\n",
    "\n",
    "all_tokens = [token for sentence in df['tokens'] for token in sentence]\n",
    "all_tags = [tag for tags in df['pos'] for tag in tags]\n",
    "\n",
    "word2idx = {word: i+2 for i, word in enumerate(set(all_tokens))}\n",
    "word2idx[\"<PAD>\"] = 0\n",
    "word2idx[\"<UNK>\"] = 1\n",
    "\n",
    "tag2idx = {tag: i for i, tag in enumerate(set(all_tags))}\n",
    "idx2tag = {i: tag for tag, i in tag2idx.items()}\n",
    "\n",
    "df['token_ids'] = df['tokens'].apply(lambda x: [word2idx.get(word, 1) for word in x])\n",
    "df['tag_ids'] = df['pos'].apply(lambda x: [tag2idx[tag] for tag in x])\n",
    "\n",
    "train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "22a2b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PosDataset(Dataset):\n",
    "    def __init__(self, data, max_len=20):\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data.iloc[idx]['token_ids']\n",
    "        tags = self.data.iloc[idx]['tag_ids']\n",
    "\n",
    "        pad_len = self.max_len - len(tokens)\n",
    "        tokens = tokens + [0]*pad_len\n",
    "        tags = tags + [-1]*pad_len\n",
    "\n",
    "        return torch.tensor(tokens), torch.tensor(tags)\n",
    "\n",
    "train_dataset = PosDataset(train_data)\n",
    "val_dataset = PosDataset(val_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "226a038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "        tag_space = self.fc(lstm_out)\n",
    "        return tag_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12210400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 660.4577\n",
      "Epoch 2, Loss: 300.7637\n",
      "Epoch 3, Loss: 228.2632\n",
      "Epoch 4, Loss: 184.7726\n",
      "Epoch 5, Loss: 151.9566\n",
      "Epoch 6, Loss: 125.0630\n",
      "Epoch 7, Loss: 103.1242\n",
      "Epoch 8, Loss: 85.1534\n",
      "Epoch 9, Loss: 70.1089\n",
      "Epoch 10, Loss: 57.4520\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BiLSTMTagger(len(word2idx), len(tag2idx)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for tokens, tags in train_loader:\n",
    "        tokens, tags = tokens.to(device), tags.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(tokens)\n",
    "        outputs = outputs.view(-1, len(tag2idx))\n",
    "        tags = tags.view(-1)\n",
    "\n",
    "        loss = criterion(outputs, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a75c4a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, sentence):\n",
    "    model.eval()\n",
    "    tokens = tokenize(sentence)\n",
    "    token_ids = [word2idx.get(tok, 1) for tok in tokens]\n",
    "    padded = token_ids + [0]*(20 - len(token_ids))\n",
    "    input_tensor = torch.tensor([padded]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    preds = torch.argmax(output, dim=-1)[0][:len(tokens)]\n",
    "    return list(zip(tokens, [idx2tag[int(i)] for i in preds]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e34acfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Vaxtımız', 'nn'), ('azdır', 'ad'), ('bu', 'un'), ('məsələni', 'nn'), ('bitirək', 'un')]\n",
      "[('Kompüterimdə', 'nn'), ('problemlər', 'nn'), ('var', 'un')]\n",
      "[('Sabah', 'un'), ('dərsə', 'nn'), ('getməliyik', 'un')]\n",
      "[('Bu', 'un'), ('işi', 'nn'), ('xoşlamıram', 'un')]\n"
     ]
    }
   ],
   "source": [
    "print(predict(model, \"Vaxtımız azdır, bu məsələni bitirək.\"))\n",
    "print(predict(model, \"Kompüterimdə problemlər var.\"))\n",
    "print(predict(model, \"Sabah dərsə getməliyik.\"))\n",
    "print(predict(model, \"Bu işi xoşlamıram.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
